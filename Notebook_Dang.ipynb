{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "Notebook_Dang.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AvNlUOoNr6ml",
        "Su6qGwimr6nk",
        "7FTvD42Jr6nm"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyenhaidang94/text_mining/blob/master/Notebook_Dang.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_QPU-Y7QlZT",
        "colab_type": "text"
      },
      "source": [
        "<center><b>Hai Dang Nguyen works with Blinda Elma Di</b></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7GLVf8lr6lJ",
        "colab_type": "text"
      },
      "source": [
        "# TP : Sentiment analysis on IMDB movie reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrDTod6Rr6lK",
        "colab_type": "text"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "1. Implement a simple way to represent text data - Bag of words\n",
        "2. Implement a basic statistical learning model - Bayesian Naive\n",
        "3. Use these representations and this model for a sentiment analysis task.\n",
        "4. Implement different ways of obtaining dense representations of the same data\n",
        "5. Use a logistic regression model to train a classifier on these new representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVLMliifr6lM",
        "colab_type": "text"
      },
      "source": [
        "## Necessary dependancies\n",
        "\n",
        "We will need the following packages:\n",
        "- The Machine Learning API Scikit-learn : http://scikit-learn.org/stable/install.html\n",
        "- The Natural Language Toolkit : http://www.nltk.org/install.html\n",
        "\n",
        "Both are available with Anaconda: https://anaconda.org/anaconda/nltk and https://anaconda.org/anaconda/scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGdnKjrksHXR",
        "colab_type": "code",
        "outputId": "64b81587-3ef1-4b35-ed66-8788202c6dad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# to be deleted\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhMCkvaYshBK",
        "colab_type": "code",
        "outputId": "eb363bdf-9976-4170-83cc-2b4ad38fc559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# to be deleted\n",
        "%cd \"drive/My Drive/Master/TPT/S2P4/SD214 - Text Mining/tp\"\n",
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Master/TPT/S2P4/SD214 - Text Mining/tp\n",
            "\u001b[0m\u001b[01;34mdata\u001b[0m/  data.joblib  Main_Notebook.ipynb  Notebook_Dang.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RoPjTLgr6lN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from glob import glob\n",
        "import os.path as op\n",
        "import joblib\n",
        "import re \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN0UcdTPr6lT",
        "colab_type": "text"
      },
      "source": [
        "## Loading data\n",
        "\n",
        "We retrieve the textual data in the variable *texts*.\n",
        "\n",
        "The labels are retrieved in the variable $y$ - it contains *len(texts)* of them: $0$ indicates that the corresponding review is negative while $1$ indicates that it is positive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RXIUd51r6lU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We get the files from the path: ./data/imdb1/neg for negative reviews, and ./data/imdb1/pos for positive reviews\n",
        "filenames_neg = sorted(glob(op.join('.', 'data', 'imdb1', 'neg', '*.txt')))\n",
        "filenames_pos = sorted(glob(op.join('.', 'data', 'imdb1', 'pos', '*.txt')))\n",
        "\n",
        "# Each files contains a review that consists in one line of text: we put this string in two lists, that we concatenate\n",
        "# This number of documents may be high for most computers: we can select a fraction of them (here, one in k)\n",
        "# Use an even number to keep the same number of positive and negative reviews\n",
        "# \n",
        "texts_neg = [open(f, encoding=\"utf8\").read() for f in filenames_neg]\n",
        "texts_pos = [open(f, encoding=\"utf8\").read() for f in filenames_pos]\n",
        "texts = texts_neg + texts_pos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OV22rcwczLoH",
        "colab_type": "code",
        "outputId": "74966563-9d8c-4605-83ac-f61db8e7c82b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The first half of the elements of the list are string of negative reviews, and the second half positive ones\n",
        "# We create the labels, as an array of [1,len(texts)], filled with 1, and change the first half to 0\n",
        "y = np.ones(len(texts), dtype=np.int)\n",
        "y[:len(texts_neg)] = 0.\n",
        "\n",
        "print(\"%d documents\" % len(texts))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000 documents\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hue7xUKDHva7",
        "colab_type": "code",
        "outputId": "ca5af494-0929-4ffe-c839-b8df37c392f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# to be deleted\n",
        "texts = joblib.load(\"data.joblib\")\n",
        "y = np.ones(len(texts), dtype=np.int)\n",
        "y[:12500] = 0.\n",
        "print(\"%d documents\" % len(texts))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000 documents\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RiuKSIY02wO",
        "colab_type": "code",
        "outputId": "88d3cc1f-0a97-47ef-930a-d2ff2bd8ce19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# This number of documents may be high for most computers: we can select a fraction of them (here, one in k)\n",
        "# Use an even number to keep the same number of positive and negative reviews\n",
        "k = 10\n",
        "texts_reduced = texts[0::k]\n",
        "y_reduced = y [0::k]\n",
        "\n",
        "print('Number of documents:', len(texts_reduced))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of documents: 2500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5ZhUWfkr6ld",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayesian \n",
        "\n",
        "## Main idea\n",
        "\n",
        "A movie review is in fact a list of words $s = (w_1, ..., w_N)$, and we try to find the associated class $c$ - which in our case may be $c = 0$ or $c = 1$. The objective is thus to find for each review $s$ the class $\\hat{c}$ maximizing the conditional probability **$P(c|s)$** : \n",
        "\n",
        "$$\\hat{c} = \\underset{c}{\\mathrm{argmax}}\\, P(c|s) = \\underset{c}{\\mathrm{argmax}}\\,\\frac{P(s|c)P(c)}{P(s)}$$\n",
        "\n",
        "**Hypothesis : P(s) is constant for each class** :\n",
        "\n",
        "$$\\hat{c} = \\underset{c}{\\mathrm{argmax}}\\,\\frac{P(s|c)P(c)}{P(s)} = \\underset{c}{\\mathrm{argmax}}\\,P(s|c)P(c)$$\n",
        "\n",
        "**Naive hypothesis : the variables (words) of a review are independant between themselves** : \n",
        "\n",
        "$$P(s|c) = P(w_1, ..., w_N|c)=\\Pi_{i=1..N}P(w_i|c)$$\n",
        "\n",
        "We will therefore be able to use the reviews at our disposal to **estimate the probabilities $P(w|c)$ for each word $w$ given the two classes $c$**. These reviews will allow us to learn how to evaluate the \"compatibility\" between words and classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T0uDxeZr6le",
        "colab_type": "text"
      },
      "source": [
        "## General view\n",
        "\n",
        "### Training: Estimating the probabilities\n",
        "\n",
        "For each word $w$ in the vocabulary $V$, $P(w|c)$ is the number of occurrences of $w$ in all reviews of class $c$, divided by the total number of occurrences in $c$. If we note $T(w,c)$ this number of occurrences, we get:\n",
        "\n",
        "$$P(w|c) = \\text{Frequency of }w\\text{ in }c = \\frac{T(w,c)}{\\sum_{w' \\in V} T(w',c)}$$\n",
        "\n",
        "### Test: Calculating scores\n",
        "\n",
        "To facilitate the calculations and to avoid *underflow* and approximation errors, we use the log-sum trick, and we pass the equation into log-probabilities : \n",
        "\n",
        "$$ \\hat{c} = \\underset{c}{\\mathrm{argmax}} P(c|s) = \\underset{c}{\\mathrm{argmax}} \\left[ \\mathrm{log}(P(c)) + \\sum_{i=1..N}log(P(w_i|c)) \\right] $$\n",
        "\n",
        "### Laplace smoothing\n",
        "\n",
        "A word that does not appear in a document has a probability of zero: this will cause issues with the logarithm. So we keep a very small part of the probability mass that we redistribute with the *Laplace smoothing*: \n",
        "\n",
        "$$P(w|c) = \\frac{T(w,c) + 1}{\\sum_{w' \\in V} T(w',c) + 1}$$\n",
        "\n",
        "There are other smoothing methods, generally suitable for other, more complex applications. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rS9CK7Kr6lf",
        "colab_type": "text"
      },
      "source": [
        "## Adapted representation of documents\n",
        "\n",
        "Our statistical model, like most models applied to textual data, uses counts of word occurrences in a document. Thus, a very convenient way to represent a document is to use a Bag-of-Words (BoW) vector, containing the counts of each word (regardless of their order of occurrence) in the document. \n",
        "\n",
        "If we consider the set of all the words appearing in our $T$ training documents, which we note $V$ (Vocabulary), we can create **an index**, which is a bijection associating to each $w$ word an integer, which will be its position in $V$. \n",
        "\n",
        "Thus, for a document extracted from a set of documents containing $|V|$ different words, a BoW representation will be a vector of size $|V|$, whose value at the index of a word $w$ will be its number of occurrences in the document. \n",
        "\n",
        "We can use the **CountVectorizer** class from scikit-learn to better understand:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkjxiUWIr6lg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR7RmHOpr6lm",
        "colab_type": "code",
        "outputId": "0bf41e76-b45d-4b71-883e-0c5df6497a72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "corpus = ['I walked down down the boulevard',\n",
        "          'I walked down the avenue',\n",
        "          'I ran down the boulevard',\n",
        "          'I walk down the city',\n",
        "          'I walk down the the avenue']\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "Bow = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(vectorizer.get_feature_names())\n",
        "Bow.toarray()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['avenue', 'boulevard', 'city', 'down', 'ran', 'the', 'walk', 'walked']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 2, 0, 1, 0, 1],\n",
              "       [1, 0, 0, 1, 0, 1, 0, 1],\n",
              "       [0, 1, 0, 1, 1, 1, 0, 0],\n",
              "       [0, 0, 1, 1, 0, 1, 1, 0],\n",
              "       [1, 0, 0, 1, 0, 2, 1, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Gqv7ypOr6lr",
        "colab_type": "text"
      },
      "source": [
        "We display the list containing the words ordered according to their index (Note that words of 2 characters or less are not counted)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8avnN7v6r6lu",
        "colab_type": "text"
      },
      "source": [
        "## Detail: training\n",
        "\n",
        "The idea is to extract the number of occurrences $T(w,c)$ for each word $w$ and each class $c$, which will make it possible to calculate the matrix of conditional probabilities $\\pmb{P}$ such that: $$\\pmb{P}_{w,c} = P(w|c)$$\n",
        "\n",
        "Note that the number of occurrences $T(w,c)$ can be easily obtained from the BoW representations of all documents !\n",
        "\n",
        "### Procedure:\n",
        "\n",
        "- Extract the vocabulary $V$ and counts $T(w,c)$ for each of the words $w$ and classes $c$, from a set of documents.\n",
        "- Calculate the a priori probabilities of the classes $P(c) = \\frac{\\sum_{w \\in V} T(w,c)}{\\sum_{c \\in C} \\sum_{w \\in V} T(w,c)}$\n",
        "- Calculate the conditional **smoothed** probabilities $P(w|c) = \\frac{T(w,c) + 1}{\\sum_{w' \\in V} T(w',c) + 1}$.\n",
        "\n",
        "## Detail: test\n",
        "\n",
        "We now know the conditional probabilities given by the $\\pmb{P}$ matrix. \n",
        "Now we must obtain $P(s|c)$ for the current document. This quantity is obtained using a simple calculation involving the BoW representation of the document and $\\pmb{P}$.\n",
        "\n",
        "### Procedure:\n",
        "\n",
        "- For each of the classes $c$,\n",
        "    - $Score(c) = \\log P(c)$\n",
        "    - For each word $w$ in the document to be tested:\n",
        "        - $Score(c) += \\log P(w|c)$\n",
        "- Return $argmax_{c \\in C} Score(c)$ \n",
        "\n",
        "Translated with www.DeepL.com/Translator (free version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LayPdzdr6lv",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing the text: get the BoW representations ##\n",
        "\n",
        "The first thing to do is to turn the review from a string into a list of words. The simplest method is to divide the string according to spaces with the command:\n",
        "``text.split()``\n",
        "\n",
        "But we must also be careful to remove special characters that may not have been cleaned up (such as HTML tags if the data was obtained from web pages). Since we're going to count words, we'll have to build a list of tokens appearing in our data. In our case, we'd like to reduce this list and make it uniform (ignore capitalization, punctuation, and the shortest words). \n",
        "We will therefore use a function adapted to our needs - but this is a job that we generally don't need to do ourselves, since there are many tools already adapted to most situations. \n",
        "For text cleansing, there are many scripts, based on different tools (regular expressions, for example) that allow you to prepare data. The division of the text into words and the management of punctuation is handled in a step called *tokenization*; if needed, a python package like NLTK contains many different *tokenizers*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO52SpsTr6lw",
        "colab_type": "code",
        "outputId": "9ba27bab-ae2f-4da7-c664-c15ed8925fe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# We might want to clean the file with various strategies:\n",
        "def clean_and_tokenize(text):\n",
        "    \"\"\"\n",
        "    Cleaning a document with:\n",
        "        - Lowercase        \n",
        "        - Removing numbers with regular expressions\n",
        "        - Removing punctuation with regular expressions\n",
        "        - Removing other artifacts\n",
        "    And separate the document into words by simply splitting at spaces\n",
        "    Params:\n",
        "        text (string): a sentence or a document\n",
        "    Returns:\n",
        "        tokens (list of strings): the list of tokens (word units) forming the document\n",
        "    \"\"\"        \n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove numbers\n",
        "    text = re.sub(r\"[0-9]+\", \"\", text)\n",
        "    # Remove punctuation\n",
        "    REMOVE_PUNCT = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
        "    text = REMOVE_PUNCT.sub(\"\", text)\n",
        "    # Do not remove small words (1 and statistique2 characters)\n",
        "    # text = re.sub(r\"\\b\\w{1,2}\\b\", \"\", text)\n",
        "    # Remove HTML artifacts specific to the corpus we're going to work with\n",
        "    REPLACE_HTML = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
        "    text = REPLACE_HTML.sub(\" \", text)\n",
        "    # Remove special chars\n",
        "    SPECIAL_CHARS = re.compile(\"[\\+\\-\\\\\\/\\^<>@#$%&\\*=_`]\")\n",
        "    text = SPECIAL_CHARS.sub(\"\", text)\n",
        "\n",
        "    tokens = text.split()        \n",
        "    return tokens\n",
        "\n",
        "# Or we might want to use an already-implemented tool. The NLTK package has a lot of very useful text processing tools, among them various tokenizers\n",
        "# Careful, NLTK was the first well-documented NLP package, but it might be outdated for some uses. Check the documentation !\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "corpus_raw = \"I walked down down the boulevard. I walked down the avenue. I ran down the boulevard. I walk down the city. I walk down the the avenue.\"\n",
        "print(clean_and_tokenize(corpus_raw))\n",
        "print(word_tokenize(corpus_raw))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "['i', 'walked', 'down', 'down', 'the', 'boulevard', 'i', 'walked', 'down', 'the', 'avenue', 'i', 'ran', 'down', 'the', 'boulevard', 'i', 'walk', 'down', 'the', 'city', 'i', 'walk', 'down', 'the', 'the', 'avenue']\n",
            "['I', 'walked', 'down', 'down', 'the', 'boulevard', '.', 'I', 'walked', 'down', 'the', 'avenue', '.', 'I', 'ran', 'down', 'the', 'boulevard', '.', 'I', 'walk', 'down', 'the', 'city', '.', 'I', 'walk', 'down', 'the', 'the', 'avenue', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHjZe7IzD5cX",
        "colab_type": "text"
      },
      "source": [
        "**Comment**: The tokens obtained by two functions are quite different. The tokens obtained by the function word_tokenize of nltk still contain punctions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l71ojSir6l1",
        "colab_type": "text"
      },
      "source": [
        "Function **to be completed**. It takes as input a list of documents (each in the form of a string) and returns, as in the example using ``CountVectorizer``:\n",
        "- A vocabulary that associates, to each word encountered, an index\n",
        "- A matrix, with rows representing documents and columns representing words indexed by the vocabulary. In position $(i,j)$, one should have the number of occurrences of the word $j$ in the document $i$.\n",
        "\n",
        "The vocabulary, which was in the form of a *list* in the previous example, can be returned in the form of a *dictionary* whose keys are the words and values are the indices. Since the vocabulary lists the words in the corpus without worrying about their number of occurrences, it can be built up using a set (in python). \n",
        "Of course, we can use the function ``clean_and_tokenize'' to transform the strings into a list of words. \n",
        "\n",
        "##### Some pointers for beginners in Python :\n",
        "\n",
        "- ```my_list.append(value)``` : put the variable ```value``` at the end of the list ```my_list```\n",
        "\n",
        "-  ```words = set()``` : create a set, which is a list of unique values \n",
        "\n",
        "- ```words.union(my_list)``` : extend the set ```words```\n",
        "\n",
        "- ```dict(zip(keys, values)))``` : create a dictionnary\n",
        "\n",
        "- ```for k, text in enumerate(texts)``` : syntax for a loop with the index, ```texts``` begin a list (of texts !)\n",
        "\n",
        "- ```len(my-list)``` : length of the list ```my_list```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3NxSVKXr6l3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_words(texts):\n",
        "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    texts : list of str\n",
        "        The texts\n",
        "    Returns\n",
        "    -------\n",
        "    vocabulary : dict\n",
        "        A dictionary that points to an index in counts for each word.\n",
        "    counts : ndarray, shape (n_samples, n_features)\n",
        "        The counts of each word in each text.\n",
        "    \"\"\"\n",
        "    words_set = set()\n",
        "    words_list = []\n",
        "    for text in texts:\n",
        "      # tokenization\n",
        "      words = clean_and_tokenize(text)\n",
        "      words_list.append(words)\n",
        "      words_set = words_set.union(words)\n",
        "    words_set = sorted(words_set)\n",
        "    vocabulary = dict(zip(words_set, np.arange(len(words_set))))\n",
        "\n",
        "    counts = np.zeros((len(texts), len(vocabulary)), dtype=int)\n",
        "    for i in range(len(texts)):\n",
        "      for word in words_list[i]:\n",
        "        counts[i, vocabulary[word] ] += 1\n",
        "    \n",
        "    return vocabulary, counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkO0JmuYr6l8",
        "colab_type": "code",
        "outputId": "b0966170-ac3c-4224-e025-05a79f47bd17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "Voc, X = count_words(corpus)\n",
        "print(Voc)\n",
        "print(X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'avenue': 0, 'boulevard': 1, 'city': 2, 'down': 3, 'i': 4, 'ran': 5, 'the': 6, 'walk': 7, 'walked': 8}\n",
            "[[0 1 0 2 1 0 1 0 1]\n",
            " [1 0 0 1 1 0 1 0 1]\n",
            " [0 1 0 1 1 1 1 0 0]\n",
            " [0 0 1 1 1 0 1 1 0]\n",
            " [1 0 0 1 1 0 2 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPmW3MZHr6mA",
        "colab_type": "text"
      },
      "source": [
        "## Naive Bayesian \n",
        "\n",
        "Empty class: functions **to be completed** : \n",
        "\n",
        "```python\n",
        "def fit(self, X, y)\n",
        "``` \n",
        "**Training**: will learn a statistical model based on the representations $X$ corresponding to the labels $y$.\n",
        "Here, $X$ contains representations obtained as the output of ```count_words```. You can complete the function using the procedure detailed above. \n",
        "\n",
        "Note: the smoothing is not necessarily done with a $1$ - it can be done with a positive value $\\alpha$, which we can implement as an argument of the class \"NB\".\n",
        "\n",
        "```python\n",
        "def predict(self, X)\n",
        "```\n",
        "**Testing**: will return the labels predicted by the model for other representations $X$.\n",
        "\n",
        "\n",
        "\n",
        "To facilitate the procedure, we will take half of the $X$ matrix obtained above to train the model, and the other half to evaluate it. **Important**: this is not realistic - usually only the training data is available when creating the vocabulary and training the model. Thus, it is possible that the evaluation data may contain *unknown* words. This is something that can easily be dealt with by dedicating a clue to all the words encountered that are not contained in the vocabulary - but there are many more complex methods for successfully using those words that the model did not encounter in training. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e7CFwKrr6mB",
        "colab_type": "text"
      },
      "source": [
        "##### Some pointers for beginners in Python :\n",
        "\n",
        "Use the Numpy API to work with arrays:\n",
        "\n",
        "\n",
        "- ```X.shape``` : for a ```numpy.array```, return the dimension of the tensor\n",
        "\n",
        "- ```np.zeros((dim_1, dim_2,...))``` : create a tensor filled with zeros\n",
        "\n",
        "- ```np.sum(X, axis = n)``` : sum the tensor over the axis n\n",
        "\n",
        "- ```np.mean(X, axis = n)```\n",
        "\n",
        "- ```np.argmax(X, axis = n)```\n",
        "\n",
        "- ```np.log(X)```\n",
        "\n",
        "- ```np.dot(X_1, X_1)``` : Matrix multiplication"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwraPm8Ir6mD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NB(BaseEstimator, ClassifierMixin):\n",
        "    # Class arguments allow to inherit from sklearn classes\n",
        "    def __init__(self, alpha=1.0):\n",
        "        # alpha is the smoothing parameter\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        vocab_size = X.shape[1]\n",
        "        all_occurrences = np.sum(X)\n",
        "        X0 = X[y==0]\n",
        "        X1 = X[y==1]\n",
        "        # p(c)\n",
        "        self.p_c0_ = np.sum(X0)/all_occurrences\n",
        "        self.p_c1_ = np.sum(X1)/all_occurrences\n",
        "        \n",
        "        # p(w|c)\n",
        "        self.p_wc0_ = (np.sum(X0, axis=0)+1)/(np.sum(X0)+vocab_size)\n",
        "        self.p_wc1_ = (np.sum(X1, axis=0)+1)/(np.sum(X1)+vocab_size)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        n = X.shape[0]\n",
        "        log_pwc0 = np.log(self.p_wc0_).reshape((-1,1))\n",
        "        log_pwc1 = np.log(self.p_wc1_).reshape((-1,1))\n",
        "        log_pwc = np.hstack((log_pwc0, log_pwc1))\n",
        "        \n",
        "        log_pc0_vector = np.repeat(np.log(self.p_c0_), n).reshape(-1,1)\n",
        "        log_pc1_vector = np.repeat(np.log(self.p_c1_), n).reshape(-1,1)\n",
        "        log_pc = np.hstack((log_pc0_vector,log_pc1_vector))\n",
        "        \n",
        "        scores = log_pc + X.dot(log_pwc)\n",
        "        result = np.argmax(scores, axis=1)\n",
        "        return result\n",
        "\n",
        "    def score(self, X, y):\n",
        "        return np.mean(self.predict(X) == y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEoLIg2Gr6mJ",
        "colab_type": "text"
      },
      "source": [
        "## Experimentation\n",
        "\n",
        "We use half the data for training, and the other half for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z34IBH35r6mK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "voc, X = count_words(texts_reduced)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "xLWh4Gyor6mO",
        "colab_type": "code",
        "outputId": "da03bcea-7242-40a4-98c8-9625e5cbedb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "nb = NB()\n",
        "nb.fit(X[::2], y_reduced[::2])\n",
        "print(\"Score in test set:\", nb.score(X[1::2], y_reduced[1::2]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score in test set: 0.7808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCy9kC-or6mS",
        "colab_type": "text"
      },
      "source": [
        "## Cross-validation \n",
        "\n",
        "With the function *cross_val_score* from scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXdGeTEpr6mT",
        "colab_type": "code",
        "outputId": "fe669582-e4c4-4d81-d7cc-c4c7e4d1d272",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "scores = cross_val_score(nb, X, y_reduced, cv=5)\n",
        "print('Cross-validation score: %s (std %s)' % (np.mean(scores), np.std(scores)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross-validation score: 0.7752000000000001 (std 0.023718347328597726)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o14xyYdcr6mX",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating performances: \n",
        "\n",
        "**What are the strengths and weaknesses of this system? How can they be remedied?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNXmjEXt1cLG",
        "colab_type": "text"
      },
      "source": [
        "The system is easy to understand and implement. However, it has three weaknesses:\n",
        "* Firstly, for the Naive Bayes classifier, it assumpts that words in a document are independent. In reality, this is not true. To improve this point, we can consider other classifier, for example: logistic regression, SVM.\n",
        "* Secondly, for the representation of document, the word count approach considers the same importance for all words in a document. Indeed, in a document, some words are more important than the others. To improve this point, we can use the Tf-idf approach.\n",
        "* Thirdly, also for the representation of document, the word count approach does not consider the context that a word appear in a document. In fact, in different context, a word can help different meaning. We can take this point into account by considering a word in a context, for example, a sentence or considering some words before and after this word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4F60ltBr6mY",
        "colab_type": "text"
      },
      "source": [
        "## To go further: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INtbvdibr6mY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLM6sJhsr6me",
        "colab_type": "text"
      },
      "source": [
        "## Scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8QUqvLmr6mf",
        "colab_type": "text"
      },
      "source": [
        "### Improving representations\n",
        "\n",
        "We use the function \n",
        "```python\n",
        "CountVectorizer\n",
        "``` \n",
        "of scikit-learn. It will allow us to easily improve our BoW representations.\n",
        "\n",
        "#### Tf-idf:\n",
        "\n",
        "This is the product of the frequency of the term (TF) and its inverse frequency in documents (IDF).\n",
        "This method is usually used to measure the importance of a term $i$ in a document $j$ relative to the rest of the corpus, from a matrix of occurrences $ words \\times documents$. Thus, for a matrix $\\mathbf{T}$ of $|V|$ terms and $D$ documents:\n",
        "$$\\text{TF}(T, w, d) = \\frac{T_{w,d}}{\\sum_{w'=1}^{|V|} T_{w',d}}$$\n",
        "\n",
        "$$\\text{IDF}(T,w) = \\log\\left(\\frac{D}{|\\{d: T_{w,d} > 0\\}|}\\right)$$\n",
        "\n",
        "$$\\text{TF-IDF}(T, w, d) = \\text{TF}(X, w, d) \\cdot \\text{IDF}(T, w)$$\n",
        "\n",
        "It can be adapted to our case by considering that the context of the second word is the document. However, TF-IDF is generally better suited to low-density matrices, since it will penalize terms that appear in a large part of the documents. \n",
        "    \n",
        "#### Do not take into account words that are too frequent:\n",
        "\n",
        "You can use the argument\n",
        "```python\n",
        "max_df=1.0\n",
        "```\n",
        "to change the amount of words taken into account. \n",
        "\n",
        "#### Try different granularities:\n",
        "\n",
        "Rather than just counting words, we can count sequences of words - limited in size, of course. \n",
        "We call a sequence of $n$ words a $n$-gram: let's try using 2 and 3-grams (bi- and trigrams).\n",
        "We can also try to use character sequences instead of word sequences.\n",
        "\n",
        "We will be interested in the options \n",
        "```python\n",
        "analyze='word'\n",
        "```\n",
        "and \n",
        "```python\n",
        "ngram_range=(1, 2)\n",
        "```\n",
        "which we'll change to alter the granularity. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_LvMX9OIr6mg",
        "colab_type": "code",
        "outputId": "01b006aa-4877-4f78-ad9c-482f424cbd1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "## We can define a pipeline, with which we can experiment.\n",
        "\n",
        "pipeline_base = Pipeline([\n",
        "    ('vect', CountVectorizer(analyzer='word', stop_words=None, tokenizer=clean_and_tokenize)),\n",
        "    ('clf', MultinomialNB()),\n",
        "])\n",
        "scores = cross_val_score(pipeline_base, texts_reduced, y_reduced, cv=5)\n",
        "print(\"Classification score: {} (std {})\".format(np.mean(scores), np.std(scores)))\n",
        "\n",
        "pipeline_tf_idf = Pipeline([\n",
        "    ('vect', TfidfVectorizer(analyzer='word', ngram_range=(1,1), tokenizer=clean_and_tokenize)),\n",
        "    ('clf', MultinomialNB()),\n",
        "])\n",
        "scores = cross_val_score(pipeline_tf_idf, texts_reduced, y_reduced, cv=5)\n",
        "print(\"Classification score tf-idf: {} (std {})\".format(np.mean(scores), np.std(scores)))\n",
        "\n",
        "pipeline_maxdf = Pipeline([\n",
        "    ('vect', TfidfVectorizer(analyzer='word', ngram_range=(1,1), max_df=0.99, tokenizer=clean_and_tokenize)),\n",
        "    ('clf', MultinomialNB()),\n",
        "])\n",
        "scores = cross_val_score(pipeline_maxdf, texts_reduced, y_reduced, cv=5)\n",
        "print(\"Classification score without most frequent words: {} (std {})\".format(np.mean(scores), np.std(scores)))\n",
        "\n",
        "pipeline_bigram = Pipeline([\n",
        "    ('vect', TfidfVectorizer(analyzer='word', ngram_range=(2,2), tokenizer=clean_and_tokenize)),\n",
        "    ('clf', MultinomialNB()),\n",
        "])\n",
        "scores = cross_val_score(pipeline_bigram, texts_reduced, y_reduced, cv=5)\n",
        "print(\"Classification score bigram: {} (std {})\".format(np.mean(scores), np.std(scores)))\n",
        "\n",
        "pipeline_trigram = Pipeline([\n",
        "    ('vect', TfidfVectorizer(analyzer='word', ngram_range=(3,3), tokenizer=clean_and_tokenize)),\n",
        "    ('clf', MultinomialNB()),\n",
        "])\n",
        "scores = cross_val_score(pipeline_trigram, texts_reduced, y_reduced, cv=5)\n",
        "print(\"Classification score trigram: {} (std {})\".format(np.mean(scores), np.std(scores)))\n",
        "\n",
        "pipeline_char = Pipeline([\n",
        "    ('vect', TfidfVectorizer(analyzer='char')),\n",
        "    ('clf', MultinomialNB()),\n",
        "])\n",
        "scores = cross_val_score(pipeline_char, texts_reduced, y_reduced, cv=5)\n",
        "print(\"Classification score char: {} (std {})\".format(np.mean(scores), np.std(scores)))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification score: 0.776 (std 0.023426480742954094)\n",
            "Classification score tf-idf: 0.7824 (std 0.02809697492613749)\n",
            "Classification score without most frequent words: 0.7827999999999999 (std 0.027758962516635937)\n",
            "Classification score bigram: 0.8048 (std 0.024416387939250946)\n",
            "Classification score trigram: 0.7612000000000001 (std 0.015419468213917119)\n",
            "Classification score char: 0.6136 (std 0.0209914268214431)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCeTanHtdpju",
        "colab_type": "text"
      },
      "source": [
        "Comment: \n",
        "* We can see that using Tf-idf approach allows to have a better result than word count approach. However, the result varies, because the std increases.\n",
        "* For the classification without most frequent words, we choose a threshold max_df=0.99. The score doesn't change much, the std reduces by 0.001. One intepretation can be the most frequent words are not important in the corpus, removing them make the score more stable.\n",
        "* When considering bigram features, the result is better, but for trigram features, the result is worse. The intepretation can be words in these document are mostlly affected by words just before or after.\n",
        "* The result for tf-idf with character is much worse, because intuiitively, a single character usually does not have any meaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvNlUOoNr6ml",
        "colab_type": "text"
      },
      "source": [
        "### Natural Language Toolkit (NLTK)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVwq5sB_r6ml",
        "colab_type": "text"
      },
      "source": [
        "### Stemming \n",
        "\n",
        "Allows to go back to the root of a word: you can group different words around the same root, which facilitates generalization. Use:\n",
        "```python\n",
        "from nltk import SnowballStemmer\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-hlyY4Yr6mm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thH_f68Sr6mr",
        "colab_type": "text"
      },
      "source": [
        "#### Example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikk1dbMBr6ms",
        "colab_type": "code",
        "outputId": "7c5d0120-9abf-4003-e036-d18769eb3f8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "words = ['singers', 'cat', 'generalization', 'philosophy', 'psychology', 'philosopher']\n",
        "for word in words:\n",
        "    print('word : %s ; stemmed : %s' %(word, stemmer.stem(word)))#.decode('utf-8'))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word : singers ; stemmed : singer\n",
            "word : cat ; stemmed : cat\n",
            "word : generalization ; stemmed : general\n",
            "word : philosophy ; stemmed : philosophi\n",
            "word : psychology ; stemmed : psycholog\n",
            "word : philosopher ; stemmed : philosoph\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m41WFRCBr6mx",
        "colab_type": "text"
      },
      "source": [
        "#### Application:\n",
        "\n",
        "Empty class : function **to complete** \n",
        "```python\n",
        "def stem(X)\n",
        "``` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUT0s7Vor6mz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stem(X): \n",
        "    X_stem = []\n",
        "    for text in X:\n",
        "      # tokenize\n",
        "      tokens = clean_and_tokenize(text)\n",
        "      # obtain vector stemmed\n",
        "      tokens_stemmed = [stemmer.stem(token) for token in tokens]\n",
        "      # concatenate to a text stemmed\n",
        "      text_stemmed = ' '.join(tokens_stemmed)\n",
        "      X_stem.append(text_stemmed)\n",
        "    return X_stem"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyOLvkddr6m3",
        "colab_type": "code",
        "outputId": "97082bec-d3af-401a-c188-2e4d5affc0dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "texts_stemmed = stem(texts_reduced)\n",
        "voc, X = count_words(texts_stemmed)\n",
        "nb = NB()\n",
        "\n",
        "scores = cross_val_score(nb, X, y_reduced, cv=5)\n",
        "print('Classification score: %s (std %s)' % (np.mean(scores), np.std(scores)))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification score: 0.7688 (std 0.02002398561725414)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ3EDGOXg5Ju",
        "colab_type": "text"
      },
      "source": [
        "**Comment**: The classification score is worse than not using stemming (0.775), however the std is smaller. It seems that keeping the original form of words allows to have a more stable result, but on the other hand, we lose information. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_z6aOJsr6m7",
        "colab_type": "text"
      },
      "source": [
        "### Part of speech tags\n",
        "\n",
        "To generalize, we can also use the Part of Speech (POS) of the words, which will allow us to filter out information that is potentially not useful to the model. We will retrieve the POS of the words using the functions:\n",
        "```python\n",
        "from nltk import pos_tag, word_tokenize\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9ph3Hypr6m8",
        "colab_type": "code",
        "outputId": "dcafd55e-3546-432e-d19d-0d9c1f61e1ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from nltk import pos_tag, word_tokenize\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_90Gv2hdr6nA",
        "colab_type": "text"
      },
      "source": [
        "#### Example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "BLYOd9NOr6nB",
        "colab_type": "code",
        "outputId": "06cfd117-2d0b-40b9-a8a5-0fd365782bbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pos_tag(word_tokenize(('I am very happy')))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'), ('am', 'VBP'), ('very', 'RB'), ('happy', 'JJ')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX5mxyETr6nJ",
        "colab_type": "text"
      },
      "source": [
        "Details of POS tags meanings: https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heMf6dtMr6nK",
        "colab_type": "text"
      },
      "source": [
        "#### Application:\n",
        "\n",
        "Empty class : function **to complete** \n",
        "```python\n",
        "def pos_tag_filter(X, good_tags=['NN', 'VB', 'ADJ', 'RB'])\n",
        "``` \n",
        "\n",
        "Only keeps nouns, adverbs, verbs and adjectives for our model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03vzHEzVr6nK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pos_tag_filter(X, good_tags=['NN', 'VB', 'JJ', 'RB']):\n",
        "    X_pos = []\n",
        "    for text in X:\n",
        "      tokens = word_tokenize(text)\n",
        "      tokens_pos = pos_tag(tokens)\n",
        "      selected_tokens = []\n",
        "      for token, pos in tokens_pos:\n",
        "        if (pos in good_tags):\n",
        "          selected_tokens.append(token)\n",
        "      text_pos = ' '.join(selected_tokens)\n",
        "      X_pos.append(text_pos)\n",
        "    return X_pos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbG8u7f9r6nU",
        "colab_type": "code",
        "outputId": "ce0cbbfe-c272-4c7e-9572-3f7544f1d3f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "texts_POS = pos_tag_filter(texts_reduced)\n",
        "voc, X = count_words(texts_POS)\n",
        "nb = NB()\n",
        "\n",
        "scores = cross_val_score(nb, X, y_reduced, cv=5)\n",
        "print('Classification score: %s (std %s)' % (np.mean(scores), np.std(scores)))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification score: 0.808 (std 0.01145425685062105)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjTBaCyqUP9n",
        "colab_type": "text"
      },
      "source": [
        "**Comment**: In this example, we keep 4 word forms: noun, verb, adjective and adverb. We have much better result than keeping all words, and the result also more stable. Grammatically, noun, verb, adjective and advert are main parts in a sentence. In reality, to identify a sentiment in a sentence, we mostly depend on these parts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-y901_ir6nY",
        "colab_type": "text"
      },
      "source": [
        "## Using a more complex classifier?\n",
        "\n",
        "We can use scikit-learn implementations of less naive classifiers, such as logistic regression or SVM. What is the main disadvantage of this (let's imagine that, rather than a linear model, we choose to use a neural network with several hidden layers)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KImNfmIr6nZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1JgQAeClP5q",
        "colab_type": "text"
      },
      "source": [
        "In this example, we use tf-idf with bigram and pos-tagging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhBjzToJr6nd",
        "colab_type": "code",
        "outputId": "ef4c1026-0523-4368-ff64-545a02f4db7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pipeline_logistic = Pipeline([\n",
        "    ('vect', TfidfVectorizer(analyzer='word', ngram_range=(2,2), tokenizer=clean_and_tokenize)),\n",
        "    ('clf', LogisticRegression())\n",
        "])\n",
        "scores = cross_val_score(pipeline_logistic, texts_reduced, y_reduced, cv=5)\n",
        "print(\"Classification score: %s (std %s)\" % (np.mean(scores), np.std(scores)))\n",
        "\n",
        "pipeline_svm = Pipeline([\n",
        "    ('vect', TfidfVectorizer(analyzer='word', ngram_range=(2,2), tokenizer=clean_and_tokenize)),\n",
        "    ('clf', LinearSVC())\n",
        "])\n",
        "scores = cross_val_score(pipeline_svm, texts_reduced, y_reduced, cv=5)\n",
        "print(\"Classification score: %s (std %s)\" % (np.mean(scores), np.std(scores)))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification score: 0.7968000000000002 (std 0.009346657156438346)\n",
            "Classification score: 0.8096 (std 0.011482160075525829)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJfN8WrE_QrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# experiment with pos-tagging filter\n",
        "texts_POS = pos_tag_filter(texts_reduced)\n",
        "voc, X = count_words(texts_POS)\n",
        "cv_scores_logit = cross_val_score(LogisticRegression(), X, y_reduced, cv=5)\n",
        "cv_scores_lnsvc = cross_val_score(LinearSVC(), X, y_reduced, cv=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0ty-G-DAQx_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fd5a8c49-9762-42c2-b0ce-bbd7f9f64276"
      },
      "source": [
        "print(\"Classification score for logit: %s (std %s)\" % (np.mean(cv_scores_logit), np.std(cv_scores_logit)))\n",
        "print(\"Classification score for linear svc: %s (std %s)\" % (np.mean(cv_scores_lnsvc), np.std(cv_scores_lnsvc)))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification score for logit: 0.8088 (std 0.012812493902437534)\n",
            "Classification score for linear svc: 0.7804 (std 0.010071742649611347)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7xzZrLAltiD",
        "colab_type": "text"
      },
      "source": [
        "**Comment**:\n",
        "* For tf-idf with bigram, logistic regression allows to have a more stable score (std 0.009), but the score is lower. Linear SVC obtains the same score with lower std.\n",
        "* For pos-tagging filter, logistic regression has similar result, linear svc performs worse in term of score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WJOTW1mr6nh",
        "colab_type": "text"
      },
      "source": [
        "# Dense Representations \n",
        "\n",
        "##  Word Embeddings : Distributed representations via the distributional hypothesis \n",
        "\n",
        "**Goal**: We will try to obtain dense representations (as vectors of real numbers) of words (and possibly sentences). These representations are intended to be distributed: they are non-local representations. We represent an object as a combination of *features*, as opposed to the attribution of a dedicated symbol: see the founding work of Geoffrey Hinton, among others, on the subject: [Distributed Representations](https://web.stanford.edu/~jlmcc/papers/PDP/Chapter3.pdf).\n",
        "\n",
        "The term *distributed* representations is very general, but is what we are looking for. The challenge is therefore to be able to build, automatically, such representations.\n",
        "\n",
        "**Underlying idea**: It is based on the distributional hypothesis: contextual information is sufficient to obtain a viable representation of linguistic objects.\n",
        " - For a large class of cases [...] the meaning of a word is its use in the language.\" Wittgenstein (Philosophical Investigations, 43 - 1953)\n",
        " - You shall know a word by the company it keeps, Firth.\n",
        "\n",
        "Thus, a word can be characterized by the words that accompany it, via co-occurrence counts. Two words with a similar meaning will have a similar contextual distribution and are therefore more likely to appear in similar contexts. This hypothesis can be used as a justification for the application of statistics to semantics (information extraction, semantic analysis). It also allows some form of generalization: we can assume that the information we have about a word will be generalized to words with a similar distribution. \n",
        "\n",
        "**Motivation**: The goal is to obtain distributed representations in order to be able to effectively**:\n",
        "- Directly perform a semantic surface analysis.\n",
        "- Use it as a source of information for other language-related models and applications, especially for sentiment analysis. \n",
        "\n",
        "\n",
        "**Terminology**: Be careful not to confuse the idea of *distributed* and *distributional* representation. The latter generally indicates (for words) that the representation has been obtained strictly from co-occurrence counts, whereas additional information (document labels, part of speech tags, ...) can be used to build distributed representations. \n",
        "The models that allow to build these dense representations, in the form of vectors, are often called *vector spaces models*. These representations are also regularly called *word embeddings*, because the words are embedded in a vector space. In French, we often find the term *word embedding* or *lexical embedding*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7CzH4Pfr6nj",
        "colab_type": "text"
      },
      "source": [
        "## Getting representations: counts of occurrences and co-occurrences\n",
        "\n",
        "Depending on the type of corpus available, different types of distributional information can be obtained. If we have access to a collection of documents, we can thus choose to count the number of occurrences of each word in each document, to obtain a $words \\times documents$ matrix: it is on this principle that **Tf-Idf** is built. We will now look at a more general case: we have a large amount of data in text form, and we want to obtain representations of words in the form of vectors of reduced size, without the need to divide them into documents or categories. \n",
        "\n",
        "Suppose we have a corpus containing $T$ different words. We will construct a $\\mathbf{M}$ matrix of size $T \\times T$ which will contain the number of co-occurrences between words. There will be different factors to consider when constructing this matrix: \n",
        "\n",
        "- How do you define the 'context' of a word - context which will tell you what terms co-occur with that word?\n",
        "\n",
        "We can choose to use different scales: the document, the sentence, the nominal group, or simply a window of $k$ words, depending on the information we want to capture.\n",
        "\n",
        "\n",
        "- How do we quantify the importance of the counts? \n",
        "\n",
        "$\\rightarrow$ For example, we can give a decreasing weight to a co-occurrence according to the distance between the two words concerned ($\\frac{1}{d+1}$ for a separation by $d$ words).\n",
        "\n",
        "\n",
        "- Should we keep all the words that appear in the corpus? \n",
        "\n",
        "$\\rightarrow$ Usually not. We will see that for large corpora, the number $T$ of different words is huge. Second, even if the number of words is reasonable, we will have very little distributional information on the rarest words, and the representation obtained will be of poor quality. We will have to ask ourselves how to filter these words, and how to treat the words we choose not to represent.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su6qGwimr6nk",
        "colab_type": "text"
      },
      "source": [
        "#### Example:\n",
        "\n",
        "Let's look at the following text:\n",
        "\n",
        "*I walked down down the boulevard. I walked down the avenue. I ran down the boulevard. I walk down the city. I walk down the the avenue.*\n",
        "\n",
        "We choose to define the context of a word as the sentence to which it belongs, and to not use any weighting.\n",
        "We obtain the following matrix: \n",
        "\n",
        "|     *         | I | the | down | walked | boulevard | avenue | walk | ran | city |\n",
        "|---------------|---|-----|------|--------|-----------|--------|------|-----|------|\n",
        "| I             | 0 |      6 |    6 |   2 |         2 |      2 |   2 |    1 |    1 |\n",
        "| the           | 6 |      2 |    7 |   2 |         2 |      3 |   3 |    1 |    1 |\n",
        "| down          | 6 |      7 |    2 |   3 |         3 |      2 |   2 |    1 |    1 |\n",
        "| walked        | 2 |      2 |    3 |   0 |         1 |      1 |   0 |    0 |    0 |\n",
        "| boulevard     | 2 |      2 |    3 |   1 |         0 |      0 |   0 |    1 |    0 |\n",
        "| avenue        | 2 |      3 |    2 |   1 |         0 |      0 |   1 |    0 |    0 |\n",
        "| ran           | 2 |      3 |    2 |   0 |         0 |      1 |   0 |    0 |    1 |\n",
        "| walk          | 1 |      1 |    1 |   0 |         1 |      0 |   0 |    0 |    0 |\n",
        "| city          | 1 |      1 |    1 |   0 |         0 |      0 |   1 |    0 |    1 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzpZAqDrr6nl",
        "colab_type": "text"
      },
      "source": [
        "## Modifying the representations:\n",
        "\n",
        "We may want to alter the representations to obtain better features - depending on what use we will have for them.\n",
        "\n",
        "**Normalization**: Very easy: we want to cancel the influence of the magnitude of the counts on the representation.\n",
        "\n",
        "$$\\mathbf{m_{normalized}} = \\left[ \n",
        "   \\frac{m_{1}}{\\sum_{i=1}^{n}m_{i}}, \n",
        "   \\frac{m_{2}}{\\sum_{i=1}^{n}m_{i}}, \n",
        "   \\ldots\n",
        "   \\frac{m_{n}}{\\sum_{i=1}^{n}m_{i}}, \n",
        "\\right]$$\n",
        " \n",
        "**Pointwise Mutual Information**: The aim is to assess the extent to which the co-occurrence of the two terms is *unexpected*. This measure is the ratio of the joint probability of the two words and the product of their individual probabilities:\n",
        "$$\n",
        "\\text{PMI}(x,y) = \\log \\left( \\frac{P(x,y)}{P(x)P(y)} \\right)\n",
        "$$\n",
        "The joint probability of the two words corresponds to the number of times they are observed together, divided by the total number of co-occurrences in the corpus: \n",
        "$$ P(\\mathbf{M},w_{1},w_{2}) = \\frac{M_{w_{1},w_{2}}}{\\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j}} $$\n",
        "The individual probability of a word simply corresponds to its frequency, which can be calculated by counting all co-occurrences where that word appears:\n",
        "$$ P(\\mathbf{M},w) = \\frac{\\sum_{j=1}^{m} M_{w,j}}{\\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j}} $$\n",
        "Hence,\n",
        "$$ \n",
        "\\text{PMI}(\\mathbf{M},w_{1},w_{2}) = \\log  \\frac{M_{w_{1},w_{2}} \\times \\left( \\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j} \\right)}{\\left( \\sum_{j=1}^{n} M_{w_{1},j} \\right) \\times \\left( \\sum_{i=1}^{n}M_{i,w_{2}} \\right)} \n",
        "$$\n",
        "We thus calculate the discrepancy between the observation we have made in our corpus and the frequency of appearance of these terms if we consider them independent - i.e. we assume that their co-occurrence is a coincidence.\n",
        "\n",
        "The main problem with this measure is that it is not adapted to the case where no co-occurrence is observed. Since the PMI is supposed to return a positive quantity if more co-occurrences are observed than expected, and a negative quantity if fewer co-occurrences are observed, we cannot choose to replace $\\log(0)$ by $0$. A commonly used solution is to use the **Positive PMI**, which sets all negative values to $0$.\n",
        " \n",
        " $$\\text{PPMI}(\\mathbf{M},w_{1},w_{2}) = \n",
        " \\begin{cases}\n",
        " \\text{PMI}(\\mathbf{M},w_{1},w_{2}) & \\textrm{if } \\text{PMI}(\\mathbf{M},w_{1},w_{2}) > 0 \\\\\n",
        " 0 & \\textrm{otherwise}\n",
        " \\end{cases}$$\n",
        " \n",
        " **TF-IDF**: As noted earlier, this is the product of the frequency of the term (TF) and its inverse frequency in the documents (IDF). \n",
        "This method is usually used to extract the importance of a term $i$ in a document $j$ relative to the rest of the corpus, from a $terms \\times documents$ matrix. Thus, for a matrix $\\mathbf{X}$ of $n$ terms and $d$ documents: \n",
        "\n",
        " $$\\text{TF}(X, i, j) = \\frac{X_{i,j}}{\\sum_{i=1}^{t} X_{i,j}} $$\n",
        " \n",
        " $$\\text{IDF}(X, i) = \\log\\left(\\frac{d}{|\\{j : X_{i,j} > 0\\}|}\\right)$$\n",
        " \n",
        " $$\\text{TF-IDF}(X, i, j) = \\text{TF}(X, i, j) \\cdot \\text{IDF}(X, i)$$\n",
        "\n",
        "\n",
        "It can be adapted to our case by considering that the context of the second word is the document. However, TF-IDF is generally better suited to low-density matrices, since it will penalize terms that appear in a large part of the documents. Thus, applying it to the co-occurrences of the most frequent words is a priori not optimal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FTvD42Jr6nm",
        "colab_type": "text"
      },
      "source": [
        "### Co-occurences matrix : reducing the dimension\n",
        "\n",
        "#### Motivation\n",
        "\n",
        "The aim is not only to reduce the size of the data (thus, we will deal with vectors of reduced dimensions, rather than working with vectors of the size of the vocabulary) but also to highlight higher level relationships between words: by reducing their representations to the *most important* dimensions of the data, we *generalize* certain properties between words.\n",
        "\n",
        "#### Dimension reduction via SVD \n",
        "\n",
        "A matrix is a linear transformation: applying an SVD to it means decomposing our linear transformation into a product of linear transformations of different types. In fact, we will change the basis of our vector, and replace our data in a space where each of the coordinates are unchanged by the transformation carried out. Thus, we decompose the matrix $\\mathbf{M}$ into three matrices:\n",
        "\n",
        "$$ \\mathbf{M} = \\mathbf{U} \\mathbf{\\lambda} \\mathbf{V}^{\\text{T}} $$\n",
        "\n",
        "Matrices $\\mathbf{U}$, $\\mathbf{\\lambda}$, et $\\mathbf{V}$ have the following properties:\n",
        "- $\\mathbf{U}$ and $\\mathbf{V}$ are orthogonal matrices ($\\mathbf{U}^{\\text{T}} = \\mathbf{U}^{-1}$ and $\\mathbf{V}^{\\text{T}} = \\mathbf{V}^{-1}$). They contain the eigen vectors to the right and to the left of $\\mathbf{M}$.\n",
        "- $\\mathbf{\\lambda}$ is a diagonal matrix: careful, it's not necessarily square. Values on the diagonal are the eigenvalues of $\\mathbf{M}$.\n",
        "\n",
        "Thus, the *most important* dimensions correspond to the largest eigenvalues. Reducing our data to $k$ dimensions corresponds to keeping only the vectors corresponding to the first $k$ eigenvalues - and this is equivalent to taking the first $k$ vectors of the $U$ matrix. \n",
        "\n",
        "Note: When we apply this method to the matrix of $\\mathbf{M}$ counts of dimension $T \\times D$, where $\\mathbf{M}_{t,d}$ contains the number of occurrences of the word $t$ in the document $d$, we obtain the method called **Latent Semantic Analysis**, for the detection of latent (semantic) components allowing the grouping of documents.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oODOyKOr6nm",
        "colab_type": "text"
      },
      "source": [
        "### In practice: get a Vocabulary.\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "\n",
        "\n",
        "To begin, we will implement separately a function returning the vocabulary. Here we will have to be able to control its size, either by indicating a maximum number of words, or a minimum number of occurrences to take the words into account. We add, at the end, an \"unknown\" word that will replace all the words that do not appear in our \"limited\" vocabulary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGU_Vvrnr6nn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vocabulary(corpus, count_threshold=1, voc_threshold=0):\n",
        "    \"\"\"    \n",
        "    Function using word counts to build a vocabulary - can be improved with a second parameter for \n",
        "    setting a frequency threshold\n",
        "    Params:\n",
        "        corpus (list of strings): corpus of sentences\n",
        "        count_threshold (int): number of occurences necessary for a word to be included in the vocabulary\n",
        "        voc_threshold (int): maximum size of the vocabulary \n",
        "    Returns:\n",
        "        vocabulary (dictionary): keys: list of distinct words across the corpus\n",
        "                                 values: indexes corresponding to each word sorted by frequency   \n",
        "        vocabulary_word_counts (dictionary): keys: list of distinct words across the corpus\n",
        "                                             values: corresponding counts of words in the corpus\n",
        "    \"\"\"\n",
        "    word_counts = dict()\n",
        "    for text in corpus:\n",
        "      words = clean_and_tokenize(text)\n",
        "      for word in words:\n",
        "        if word in word_counts:\n",
        "          word_counts[word] += 1\n",
        "        else:\n",
        "          word_counts[word] = 1\n",
        "    # sort vocabulary by occurences\n",
        "    word_counts_sorted = sorted(word_counts.items(), key=lambda item: item[1], reverse=True)\n",
        "    # filter by min number of ocurrences, filtered_word_counts is a list of tuple\n",
        "    filtered_word_counts = []\n",
        "    filtered_out_word_counts = []\n",
        "    for k,v in word_counts_sorted:\n",
        "      if (v >= count_threshold):\n",
        "        filtered_word_counts.append((k,v))\n",
        "      else:\n",
        "        filtered_out_word_counts.append((k,v))\n",
        "\n",
        "    # filter by max vocabulary size\n",
        "    # with default value voc_threshold=0, we do nothing\n",
        "    if (voc_threshold > 0):\n",
        "      if (voc_threshold < len(filtered_word_counts)):\n",
        "        filtered_out_word_counts = filtered_out_word_counts + filtered_word_counts[voc_threshold:]\n",
        "        filtered_word_counts = filtered_word_counts[:voc_threshold]\n",
        "\n",
        "    # construct the vocabulary\n",
        "    words = [k for k,v in filtered_word_counts]\n",
        "    words.append('UNK')\n",
        "    values = [v for k,v in filtered_word_counts]\n",
        "    unk_occurences = sum([v for k,v in filtered_out_word_counts])\n",
        "    values.append(unk_occurences)\n",
        "\n",
        "    vocabulary = dict(zip(words, np.arange(len(words))))\n",
        "    vocabulary_word_counts = dict(zip(words, values))\n",
        "    return vocabulary, vocabulary_word_counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdxH8Koor6ns",
        "colab_type": "code",
        "outputId": "0cc0b4ae-550e-420e-d8f5-dc658e3bcf24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Example for testing:\n",
        "\n",
        "corpus = ['I walked down down the boulevard',\n",
        "          'I walked down the avenue',\n",
        "          'I ran down the boulevard',\n",
        "          'I walk down the city',\n",
        "          'I walk down the the avenue']\n",
        "\n",
        "voc, counts = vocabulary(corpus, count_threshold = 3)\n",
        "print(voc)\n",
        "print(counts)\n",
        "\n",
        "# We expect something like this:\n",
        "# (In this example, we don't count 'UNK' unknown words, but you can if you want to. \n",
        "# How useful it may be depends on the data -> we will use the counts later with word2vec, keep that in mind) \n",
        "#  {'down': 0, 'the': 1, 'i': 2, 'UNK': 3}\n",
        "#  {'down': 6, 'the': 6, 'i': 5, 'UNK': 0}\n",
        "\n",
        "voc, counts = vocabulary(corpus)\n",
        "print(voc)\n",
        "print(counts)\n",
        "\n",
        "# We expect something like this:\n",
        "#  {'down': 0, 'the': 1, 'i': 2, 'walked': 3, 'boulevard': 4, 'avenue': 5, 'walk': 6, 'ran': 7, 'city': 8, 'UNK': 9}\n",
        "#  {'down': 6, 'the': 6, 'i': 5, 'walked': 2, 'boulevard': 2, 'avenue': 2, 'walk': 2, 'ran': 1, 'city': 1, 'UNK': 0}"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'down': 0, 'the': 1, 'i': 2, 'UNK': 3}\n",
            "{'down': 6, 'the': 6, 'i': 5, 'UNK': 10}\n",
            "{'down': 0, 'the': 1, 'i': 2, 'walked': 3, 'boulevard': 4, 'avenue': 5, 'walk': 6, 'ran': 7, 'city': 8, 'UNK': 9}\n",
            "{'down': 6, 'the': 6, 'i': 5, 'walked': 2, 'boulevard': 2, 'avenue': 2, 'walk': 2, 'ran': 1, 'city': 1, 'UNK': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZaV0QvKr6nv",
        "colab_type": "text"
      },
      "source": [
        "#### Application to a real data set\n",
        "\n",
        "We're going to work with the **imdb** data.\n",
        "\n",
        "#### Quick study of the data\n",
        "\n",
        "We would like to get an idea of what's in these film reviews before we proceed. So we'll get the vocabulary (in full) and represent the frequencies of the words, in order (be careful, you'll have to use a logarithmic scale): we should find back Zipf's law. This will give us an idea of the size of the vocabulary we will be able to choose: it's a matter of making a compromise between the necessary resources (size of the objects in memory) and the amount of information we can get from them (rare words can bring a lot of information, but it's difficult to learn good representations of them, because they are rare!).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8B7DxGIr6nw",
        "colab_type": "code",
        "outputId": "5d2c6af9-3684-45ad-8c8e-89b84ce8461a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "# We would like to display the curve of word frequencies given their rank (index) in the vocabulary\n",
        "# vocab, word_counts = vocabulary(texts)\n",
        "# ranks = np.arange(1, len(vocab)+1)\n",
        "# counts = list(word_counts.values())\n",
        "\n",
        "k=5000\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.title('Word counts versus rank')\n",
        "plt.plot(ranks[:k], counts[:k], label='First 5000 words')\n",
        "plt.plot(ranks[k:], counts[k:], label='Remaining words')\n",
        "plt.yscale('log')\n",
        "plt.xlabel(\"Rank\")\n",
        "plt.ylabel(\"Log count\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4lnNzEzUkXb",
        "colab_type": "text"
      },
      "source": [
        "**Comment**: With the first 5000 words, the slope decrease quickly. From the words 5001 to the rest, the slope gently decreases. Therefore, we keep 5000 words in our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFBiWEgYr6n1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_5k, word_counts_5k = vocabulary(texts, 0, 5000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFdCXwKFr6n6",
        "colab_type": "code",
        "outputId": "cdf345f5-b08a-435b-c3fd-05dde6f4248e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(vocab_5k['cinema'])"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28mVkTE5r6oC",
        "colab_type": "text"
      },
      "source": [
        "We could here compute the co-occurence matrix, and then reduce its dimension. Instead, we will use two of the most popular methods used to produce dense word representations (word embeddings). These methods are very different in practice, but are conceptually close, and resemble the procedure described earlier: reducing the dimension of co-occurences metrics.\n",
        "\n",
        "## Getting a representation: commonly used algorithms\n",
        "\n",
        "The idea here is to define a set of representations ${w_{i}}_{i=1}^{V}$, of predefined dimension $d$ (here, we will work with $d = 300$), for all the words $i$ of the vocabulary $V$ - then **train** these representations to match what we want. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT_82b48r6oD",
        "colab_type": "text"
      },
      "source": [
        "### Word2Vec\n",
        "\n",
        "\n",
        "####  The skip-gram model\n",
        "\n",
        "The basic skip-gram model estimates the probabilities of a pair of words $(i, j)$ to appear together in data:\n",
        "\n",
        "$$P(j \\mid i) = \\frac{\\exp(w_{i} c_{j})}{\\sum_{j'\\in V}\\exp(w_{i} c_{j'})}$$\n",
        "\n",
        "\n",
        "where $w_{i}$ is the lign vector (of the word) $i$ and $c_{j}$ is the column vector (of a context word) $j$. The objective is to minimize the following quantity:\n",
        "\n",
        "\n",
        "$$ -\\sum_{i=1}^{m} \\sum_{k=1}^{|V|} \\textbf{1}\\{o_{i}=k\\} \\log \\frac{\\exp(w_{i} c_{k})}{\\sum_{j=1}^{|V|} \\exp(w_{i} c_{j})}$$\n",
        "\n",
        "\n",
        "where $V$ is the vocabulary.\n",
        "The inputs $w_{i}$ are the representations of the words, which are updated during training, and the output is an *one-hot* $o$ vector, which contains only one $1$ and $0$. For example, if `good` is the 47th word in the vocabulary, the output $o$ for an example or `good` is the word to predict will consist of $0$s everywhere except $1$ in the 47th position of the vector. `good` will be the word to predict when the input $w$ is a word in its context.\n",
        "We therefore obtain this output with standard softmax - we add a bias term $b$ .\n",
        "\n",
        "\n",
        "$$ o = \\textbf{softmax}(w_{a}C + b)$$\n",
        "\n",
        "\n",
        "If we use the set of representations for the whole vocabulary (the matrix $W$) as input, we get:\n",
        "\n",
        "\n",
        "$$ O = \\textbf{softmax}(WC + b)$$\n",
        "\n",
        "\n",
        "and so we come back to the central idea of all our methods: we seek to obtain word representations from co-occurrence counts. Here, we train the parameters contained in $W$ and $C$, two matrices representing the words in reduced dimension (300) so that their scalar product is as close as possible to the co-occurrences observed in the data, using a maximum likelihood objective.\n",
        "\n",
        "#### Skip-gram with negative sampling\n",
        "\n",
        "The training of the skip-gram model implies to calculate a sum on the whole vocabulary, because of the **softmax**. As soon as the size of the vocabulary increases, it becomes impossible to compute. In order to make the calculations faster, we change the objective and use the method of *negative sampling* (or, very close to it, the *noise contrastive estimation*).\n",
        "\n",
        "\n",
        "If we note $\\mathcal{D}$ the data set and we note $\\mathcal{D}'$ a set of pairs of words that are **not** in the data (and that in practice, we draw randomly), the objective is:\n",
        "\n",
        "\n",
        "$$\\sum_{i, j \\in \\mathcal{D}}-\\log\\sigma(w_{i}c_{j}) + \\sum_{i, j \\in \\mathcal{D}'}\\log\\sigma(w_{i}c_{j})$$\n",
        "\n",
        "\n",
        "where $\\sigma$ is the sigmoid activation function $\\frac{1}{1 + \\exp(-x)}$.\n",
        "A common practice is to generate pairs from $\\mathcal{D}'$ in proportion to the frequencies of the words in the training data (the so-called unigram distribution):\n",
        "\n",
        "\n",
        "$$P(w) = \\frac{\\textbf{T}(w)^{0.75}}{\\sum_{w'\\in V} \\textbf{T}(w')}$$\n",
        "\n",
        "\n",
        "Although different, this new objective function is a sufficient approximation of the previous one, and is based on the same principle. Much research has been done on this objective: for example, [Levy and Golberg 2014](http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization) shows that the objective calculates the PMI matrix shifted by a constant value. One can also see [Cotterell et al. 2017](https://aclanthology.coli.uni-saarland.de/papers/E17-2028/e17-2028) for an interpretation of the algorithm as a variant of PCA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKmN-mOwr6oE",
        "colab_type": "text"
      },
      "source": [
        "We will use the ```gensim``` library for its implementation of word2vec in python. We'll have to make a specific use of it, since we want to keep the same vocabulary as before: we'll first create the class, then get the vocabulary we generated above. \n",
        "To avoid having to put all the data in memory all at once, we define a generator, which will take all the input data and pre-process it, and return to the ```Word2Vec``` class sentence by sentence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4webh5_r6oF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Creates the Word2Vec model with the relevant parameters\n",
        "model = Word2Vec(size=300,\n",
        "                 window=5,\n",
        "                 iter=30)\n",
        "\n",
        "# Get the vocabulary from the counts we created earlier\n",
        "model.build_vocab_from_freq(word_counts_5k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctWuvng1r6oJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_generator(large_corpus):\n",
        "    for line in large_corpus:\n",
        "        yield clean_and_tokenize(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrX3BL5Gr6oO",
        "colab_type": "code",
        "outputId": "51c2c68a-fa63-411c-8d4b-4e3a10b184f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.train(preprocess_generator(texts[:]), total_examples=10, epochs=30, report_delay=1)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3821648, 5787719)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdcID0nFr6oU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W2VEmbeddings = model.wv.vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL0ZMiuir6oZ",
        "colab_type": "text"
      },
      "source": [
        "### Glove\n",
        "\n",
        "The objective defined by Glove ([Pennington et al. (2014)](http://www.aclweb.org/anthology/D/D14/D14-1162.pdf)) is to learn from the vectors $w_{i}$ and $w_{k}$ so that their scalar product corresponds to the logarithm of their **Pointwise Mutual Information**: \n",
        "\n",
        "\n",
        "$$ w_{i}^\\top w_{k} = (PMI(w_{i}, w_{k}))$$\n",
        "\n",
        "\n",
        "In the article, this objective is carefully justified by a reasoning about the operations one wants to perform with these vectors and the properties they should have - in particular, symmetry between rows and columns (see the article for more details).  \n",
        "The final goal obtained is the following, where $M$ is the co-occurrence matrix:\n",
        "\n",
        "\n",
        "$$\\sum_{i, j=1}^{|V|} f\\left(M_{ij}\\right)\n",
        "  \\left(w_i^\\top w_j + b_i + b_j - \\log M_{ij}\\right)^2$$\n",
        "  \n",
        " \n",
        "Here, $f$ is a *scaling* function that reduces the importance of the most frequent co-occurrence counts: \n",
        "\n",
        "\n",
        "$$f(x) \n",
        "\\begin{cases}\n",
        "(x/x_{\\max})^{\\alpha} & \\textrm{if } x < x_{\\max} \\\\\n",
        "1 & \\textrm{otherwise}\n",
        "\\end{cases}$$\n",
        "\n",
        "\n",
        "Usually, we choose $\\alpha=0.75$ and $x_{\\max} = 100$, although these parameters may need to be changed depending on the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byfcu7ber6oZ",
        "colab_type": "text"
      },
      "source": [
        "The following code uses the gensim API to retrieve pre-trained representations (It is normal that the loading is long)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XqNHUv3r6oa",
        "colab_type": "code",
        "outputId": "b50bea03-59d7-4da5-b008-b1a627cdafc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "loaded_glove_model = api.load(\"glove-wiki-gigaword-300\")"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyHdbA62r6oe",
        "colab_type": "text"
      },
      "source": [
        "We can extract the embedding matrix this way, and check its size:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpYHtcD2r6oh",
        "colab_type": "code",
        "outputId": "1d78ce75-274e-4d85-80e9-2e2339e37937",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loaded_glove_embeddings = loaded_glove_model.vectors\n",
        "print(loaded_glove_embeddings.shape)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(400000, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ziab0BR6r6om",
        "colab_type": "text"
      },
      "source": [
        "We can see that there are $400,000$ words represented, and that the embeddings are of size $300$. We define a function that returns, from the loaded model, the vocabulary and the embedding matrix according to the structures we used before. We add, here again, an unknown word \"UNK\" in case there are words in our data that are not part of the $400,000$ words represented here. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkLWYA1qr6on",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_glove_voc_and_embeddings(glove_model):\n",
        "    voc = {word : index for word, index in enumerate(glove_model.index2word)}\n",
        "    voc['UNK'] = len(voc)\n",
        "    embeddings = glove_model.vectors\n",
        "    return voc, embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkT_Vw0Wr6or",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaded_glove_voc, loaded_glove_embeddings = get_glove_voc_and_embeddings(loaded_glove_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8b5282Vr6ow",
        "colab_type": "text"
      },
      "source": [
        "In order to compare the representations loaded here and the ones produced with word2vec, the same vocabulary should be used. For this purpose, I reuse the following code to create a $5000$ word vocabulary from the data, and I add at the end a function that returns the matrix of representations loaded with Glove for these $5000$ words only, in the right order. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-sdlD8or6ow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_glove_adapted_embeddings(glove_model, input_voc):\n",
        "    keys = {i: glove_model.vocab.get(w, None) for w, i in input_voc.items()}\n",
        "    index_dict = {i: key.index for i, key in keys.items() if key is not None}\n",
        "    embeddings = np.zeros((len(input_voc),glove_model.vectors.shape[1]))\n",
        "    for i, ind in index_dict.items():\n",
        "        embeddings[i] = glove_model.vectors[ind]\n",
        "    return embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PCuGTqNr6o1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GloveEmbeddings = get_glove_adapted_embeddings(loaded_glove_model, vocab_5k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz8XHmJ3r6o5",
        "colab_type": "text"
      },
      "source": [
        "This function takes as input the model loaded using the Gensim API, as well as a vocabulary we created ourselves, and returns the embedding matrix from the loaded model, for the words in our vocabulary and in the right order.\n",
        "Note: unknown words are represented by a vector of zeros:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8jdTVcmr6o6",
        "colab_type": "code",
        "outputId": "d5fe4188-652b-479f-a62b-87bbfef741b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "print(GloveEmbeddings.shape)\n",
        "GloveEmbeddings[vocab_5k['UNK']]"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5001, 300)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-4ab0Q5r6o9",
        "colab_type": "text"
      },
      "source": [
        "### Comparing vectors\n",
        "\n",
        "These very large vectors can be used for a very basic semantic analysis: for example, by searching for the closest neighbors of a word. However, one must be careful with the distances used, related to certain metrics (Euclidean, Cosine) or possibly others related to belonging to sets (Matching, Jaccard). The normalization of vectors can also play a role. In any case, care must be taken not to over-interpret such results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rPRfOCzr6o-",
        "colab_type": "code",
        "outputId": "eb097191-9bee-4962-ebc3-32504547b042",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "def euclidean(u, v):\n",
        "    return np.linalg.norm(u-v)\n",
        "\n",
        "def length_norm(u):\n",
        "    return u / np.sqrt(u.dot(u))\n",
        "\n",
        "def cosine(u, v):\n",
        "    return 1.0 - length_norm(u).dot(length_norm(v))\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "def print_neighbors(distance, voc, co_oc, mot, k=10):\n",
        "    inv_voc = {id: w for w, id in voc.items()}\n",
        "    neigh = NearestNeighbors(k, algorithm='brute', metric=distance)\n",
        "    neigh.fit(co_oc) \n",
        "    dist, ind = neigh.kneighbors([co_oc[voc[mot]]])\n",
        "    print(\"Closest neighbors of %s by distance '%s': \" % (mot, distance.__name__))\n",
        "    print([[inv_voc[i] for i in s[1:]] for s in ind])\n",
        "    \n",
        "print_neighbors(euclidean, vocab_5k, W2VEmbeddings, 'good')\n",
        "print_neighbors(cosine, vocab_5k, W2VEmbeddings, 'good')\n",
        "\n",
        "print_neighbors(euclidean, vocab_5k, GloveEmbeddings, 'good')\n",
        "print_neighbors(cosine, vocab_5k, GloveEmbeddings, 'good')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Closest neighbors of good by distance 'euclidean': \n",
            "[['what', 'up', 'were', 'would', 'she', 'had', 'story', 'about', 'more']]\n",
            "Closest neighbors of good by distance 'cosine': \n",
            "[['and', 'of', 'a', 'it', 'in', 'was', 'is', 'movie', 'that']]\n",
            "Closest neighbors of good by distance 'euclidean': \n",
            "[['better', 'well', 'always', 'really', 'sure', 'way', 'so', 'but', 'excellent']]\n",
            "Closest neighbors of good by distance 'cosine': \n",
            "[['better', 'really', 'always', 'you', 'well', 'excellent', 'very', 'things', 'think']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaHfh8zYz_kA",
        "colab_type": "text"
      },
      "source": [
        "The closest neighbors of the word 'good' provided by Glove, have closer meaning than Word2Vec."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYrgDclxr6pC",
        "colab_type": "text"
      },
      "source": [
        "### Visualisation in two dimensions\n",
        "\n",
        "For visualization, we use t-SNE instead of PCA, because t-SNE is better than PCA in visualizing high dimensional data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JieyuAKmG71g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "91ebeeff-0ff9-4f91-8dc8-9b0df5c8f0d5"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE()\n",
        "Glove_Emb = tsne.fit_transform(GloveEmbeddings)\n",
        "\n",
        "words = ['bad', 'good', 'best', 'worst', 'poor', 'great',\n",
        "         'dialog', 'role', 'actor', 'camera', 'scene',\n",
        "         'film', 'movie', 'oscar', 'award']\n",
        "ind_words = [vocab_5k[w] for w in words]\n",
        "x_words = [Glove_Emb[ind,0] for ind in ind_words]\n",
        "y_words = [Glove_Emb[ind,1] for ind in ind_words]\n",
        "\n",
        "plt.figure(figsize=(9,6))\n",
        "plt.scatter(x_words, y_words)\n",
        "\n",
        "for i, w in enumerate(words):\n",
        "    plt.annotate(w, (x_words[i], y_words[i]), (x_words[i] + 0.001, y_words[i] + 0.001))\n",
        "plt.show()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy4ukDhtJJ28",
        "colab_type": "text"
      },
      "source": [
        "**Comment**: We can see that the words with similar type are potioned close together in the plot. For example, the cluster in the right side contains 'scene', 'camera', 'actor', 'movie', 'film' refers to something presence in the movie.  \n",
        "The cluster in the left side contains 'worst', 'poor', 'good', 'great', 'bad' refers to adjectives to evaluating actors or movies. We expect the word 'best' stands closer to the left cluster, but it is closer to the bottom cluster (role, oscar, award)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzK4w2Tcr6pH",
        "colab_type": "code",
        "outputId": "77716df3-878a-4a63-da57-fdcf1694506c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "tsne = TSNE()\n",
        "W2V_Emb = tsne.fit_transform(W2VEmbeddings)\n",
        "\n",
        "words = ['bad', 'good', 'best', 'worst', 'poor', 'great',\n",
        "         'dialog', 'role', 'actor', 'camera', 'scene',\n",
        "         'film', 'movie', 'oscar', 'award']\n",
        "ind_words = [vocab_5k[w] for w in words]\n",
        "x_words = [W2V_Emb[ind,0] for ind in ind_words]\n",
        "y_words = [W2V_Emb[ind,1] for ind in ind_words]\n",
        "\n",
        "plt.figure(figsize=(9,6))\n",
        "plt.scatter(x_words, y_words)\n",
        "\n",
        "for i, w in enumerate(words):\n",
        "    plt.annotate(w, (x_words[i], y_words[i]), (x_words[i] + 0.001, y_words[i] + 0.001))\n",
        "plt.show()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b61kvYpiWv2t",
        "colab_type": "text"
      },
      "source": [
        "**Comment**: The clusters obtained by Word2Vec are different from Glove's. The bottom cluster (great, bad, good, film, movie) actually refers to two topics: movie and evaluation of movie. The left cluster also refers to two topics: elements of movie and evaluation of movie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7hEzvhVr6pO",
        "colab_type": "text"
      },
      "source": [
        "## Application to sentiment analysis\n",
        "\n",
        "We will now use these representations for sentiment analysis. \n",
        "The basic model, as before, will be constructed in two steps:\n",
        "- A function to obtain vector representations of criticism, from text, vocabulary, and vector representations of words. Such a function (to be completed below) will associate to each word of a review its embeddings, and create the representation for the whole sentence by summing these embeddings.\n",
        "- A classifier will take these representations as input and make a prediction. To achieve this, we can first use logistic regression ```LogisticRegression``` from ```scikit-learn```  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI6soMNur6pP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_representations(texts, vocabulary, embeddings, np_func=np.sum):\n",
        "    \"\"\"\n",
        "    Represent the sentences as a combination of the vector of its words.\n",
        "    Parameters\n",
        "    ----------\n",
        "    texts : a list of sentences   \n",
        "    vocabulary : dict\n",
        "        From words to indexes of vector.\n",
        "    embeddings : Matrix containing word representations\n",
        "    np_func : function (default: np.sum)\n",
        "        A numpy matrix operation that can be applied columnwise, \n",
        "        like `np.mean`, `np.sum`, or `np.prod`. \n",
        "    Returns\n",
        "    -------\n",
        "    np.array, dimension `(len(texts), embeddings.shape[1])`            \n",
        "    \"\"\"\n",
        "    representations = np.zeros((len(texts), embeddings.shape[1]))\n",
        "    unk_idx = vocabulary['UNK']\n",
        "    for i in range(len(texts)):\n",
        "        words = clean_and_tokenize(texts[i])\n",
        "        indices = []\n",
        "        for word in words:\n",
        "            if vocabulary.get(word):\n",
        "                indices.append(vocabulary.get(word))\n",
        "            else:\n",
        "                indices.append(unk_idx)\n",
        "        representations[i] = np.sum(embeddings[indices,:], axis=0)\n",
        "    return representations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5svttjur6pT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Exemple with the embeddings obtained by Glove\n",
        "rep = sentence_representations(texts, vocab_5k, GloveEmbeddings)\n",
        "clf = LogisticRegression().fit(rep[::2], y[::2])\n",
        "score_glove = clf.score(rep[1::2], y[1::2])\n",
        "cv_scores_glove = cross_val_score(clf, rep, y, cv=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIMgagIFX1a-",
        "colab_type": "code",
        "outputId": "1a853ad1-402a-4dc3-eae8-aab0de904b58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Score of glove: %s\" % score_glove)\n",
        "print('Cross-validation score of Glove: %s (std %s)' % (np.mean(cv_scores_glove), np.std(cv_scores_glove)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score of glove: 0.8408\n",
            "Cross-validation score of Glove: 0.83212 (std 0.003481608823518234)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5pHffmZ_5M4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using embeddings obtained by Word2Vec\n",
        "rep2 = sentence_representations(texts, vocab_5k, W2VEmbeddings)\n",
        "\n",
        "clf2 = LogisticRegression().fit(rep2[::2], y[::2])\n",
        "score_w2v = clf2.score(rep2[1::2], y[1::2])\n",
        "\n",
        "cv_scores_w2v = cross_val_score(clf2, rep2, y, cv=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUSepfDzYFA3",
        "colab_type": "code",
        "outputId": "a79f5e2f-f20e-45a6-f9db-11b4a3d11592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Score of word2vec: %s\" % score_w2v)\n",
        "print('Cross-validation score of word2vec: %s (std %s)' % (np.mean(cv_scores_w2v), np.std(cv_scores_w2v)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score of word2vec: 0.70608\n",
            "Cross-validation score of word2vec: 0.70392 (std 0.011896621369111496)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bixpeFmsr6pZ",
        "colab_type": "text"
      },
      "source": [
        "Why can we expect that the results obtained with embeddings extracted from representations pre-trained with Gl0ve are much better than word2vec ? What would be the way to compare Gl0ve with word2vec in a 'fair' way ? Try to play with word2vec parameters to improve results !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-doEPDiXKrZ",
        "colab_type": "text"
      },
      "source": [
        "**Comment**: We expect that the results obtained with embeddings extracted from representations pre-trained with Glove are muc better than Word2vec, because Glove was trained in a larger corpus (6B tokens), a large vocabulary (400K words) (corresponding to [Standford](https://nlp.stanford.edu/projects/glove/)).  \n",
        "A fair way to compare Glove and Word2vec is training two algorithm in the same corpus."
      ]
    }
  ]
}